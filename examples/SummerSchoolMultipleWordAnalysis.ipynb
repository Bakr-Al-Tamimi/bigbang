{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This note book gives the trend of multiple words in multiple mailing lists\n",
    "\n",
    "What it does:\n",
    "-it computes and plot word counts over time, on aggregated mailing lists' data.\n",
    "-it exports emails that contains selected words \n",
    "\n",
    "Parameters to set options:\n",
    "-it can track one or more words, according to the number of words set in the variable 'checkwords' \n",
    "-it can look in one or more mailing lists, according to how many urls are set; word counts are aggregated across mls\n",
    "-it can look at literal words or at stemmed words, according to the 'stem' parameter\n",
    "\n",
    "Useful extensions:\n",
    "-export dictionary with wordcount trends on individual mailing lists\n",
    "-look at compund words (e.g. 'human rights')\n",
    "-give option to SUM word counts instead of treating words separately\n",
    "-give possibility to normalize word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sb/anaconda/envs/nllz/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bigbang.archive import load as load_archive\n",
    "from bigbang.archive import Archive\n",
    "import bigbang.mailman as mailman\n",
    "import bigbang.process as process\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pprint import pprint as pp\n",
    "import pytz\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "from itertools import repeat\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sb/anaconda/envs/nllz/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2885: FutureWarning: \n",
      "mpl_style had been deprecated and will be removed in a future version.\n",
      "Use `matplotlib.pyplot.style.use` instead.\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.mpl_style = 'default' # pandas has a set of preferred graph formatting options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sb/projects/nllz-bigbang/bigbang/bigbang/archive.py:73: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  self.data.sort(columns='Date', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#insert a list of the urls of downloaded mailing lists that you want to include in the analysis. \n",
    "#data will be merged: multiple mailing lists are treated as a unique corpus\n",
    "#e.g. urls  = [\"https://mm.icann.org/pipermail/cc-humanrights/\", \n",
    "#              \"https://mm.icann.org/pipermail/euro-board/\"]\n",
    "\n",
    "urls = [\"http://mm.icann.org/pipermail/cc-humanrights/\",\n",
    "              \"http://mm.icann.org/pipermail/euro-board/\"]\n",
    "       #\"http://mm.icann.org/pipermail/wp4/\"]\n",
    "#       \"http://mm.icann.org/pipermail/wp1/\"]\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    arch_paths =[]\n",
    "    for url in urls:\n",
    "        arch_paths.append('../archives/'+url[:-1].replace('://','_/')+'.csv')\n",
    "    archives = [load_archive(arch_path) for arch_path in arch_paths]\n",
    "except:\n",
    "    arch_paths =[]\n",
    "    for url in urls:\n",
    "        arch_paths.append('../archives/'+url[:-1].replace('//','/')+'.csv')\n",
    "        \n",
    "archives = [load_archive(arch_path) for arch_path in arch_paths]\n",
    "\n",
    "df = pd.concat([arx.data for arx in archives])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#insert a list of *single* words to be tracked e.g. checkwords = ['rights', 'economy', 'human']\n",
    "checkwords = [\"internet\",\"right\",\"human\",\"human rights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to stem or not to stem? \n",
    "#if stem is set to True, then checkwords should be stemmed words (no plurals, no suffixes, etc.)\n",
    "#if stem is set to False, then checkwords are searched for their literal spelling\n",
    "stem = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extension: filter by date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_word(text,word):\n",
    "    if not text:\n",
    "        return 0\n",
    "    \n",
    "    if len(word.split(\" \")) <= 1:\n",
    "        ## normalize the text - remove apostrophe and punctuation, lower case\n",
    "        normalized_text = re.sub(r'[^\\w]', ' ',text.replace(\"'\",\"\")).lower()\n",
    "    \n",
    "        tokenized_text = nltk.tokenize.word_tokenize(normalized_text)\n",
    "\n",
    "        if stem:\n",
    "            tokenized_text = [st.stem(t) for t in tokenized_text]\n",
    "    \n",
    "        return tokenized_text.count(word)\n",
    "    else:\n",
    "        return text.lower().count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in checkwords:\n",
    "    df[word] = df['Body'].apply(lambda x: count_word(x,word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save each email in a file based on which checkword it contains. good for doing some qualitative analysis\n",
    "\n",
    "#set the path where the data are to be saved\n",
    "path = '.'\n",
    "\n",
    "import os\n",
    "\n",
    "for word in checkwords:\n",
    "    print \"Saving data for checkword \"+word+\"...\"\n",
    "    df[df[word] > 0].to_csv(os.path.join(path,word+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Date'])\n",
    "df['Date-ordinal'] = df['Date'].apply(lambda x: x.toordinal())\n",
    "\n",
    "df_sums = df.groupby('Date-ordinal').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "for_export = df_sums.copy()\n",
    "\n",
    "dates_again = pd.Series(for_export.index,\n",
    "                  index=for_export.index).apply(lambda x: \n",
    "                                                date.fromordinal(x))\n",
    "\n",
    "for_export['Date'] = dates_again\n",
    "\n",
    "for_export.to_csv(\"word_counts_by_date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.5, 7.5))\n",
    "\n",
    "colors = 'rgbkm'\n",
    "\n",
    "window = 5\n",
    "\n",
    "for i in range(len(checkwords)):\n",
    "    smooth_sums = pd.rolling_mean(df_sums,window)\n",
    "    \n",
    "    plt.plot_date(smooth_sums.index,\n",
    "                  smooth_sums[checkwords[i]],\n",
    "                  colors[i],\n",
    "                  label=checkwords[i])\n",
    "\n",
    "plt.legend(bbox_to_anchor=(.2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
